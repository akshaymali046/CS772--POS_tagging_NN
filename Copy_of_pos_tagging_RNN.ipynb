{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkOvkjUK9VxR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from nltk.corpus import brown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import seaborn as sns"
      ],
      "metadata": {
        "id": "8MBSkXBZ-m3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "JO51VHMtBZ5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Activation\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "gdOf_66O-8FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OCMcMv9EEn_",
        "outputId": "fb9eec71-42d6-4feb-f8cc-5afa658efad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y9un4mG82pa",
        "outputId": "1ece1afe-990f-4dd6-cc84-53947a50e23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brown_corpus = brown.tagged_sents(tagset='universal')"
      ],
      "metadata": {
        "id": "Tdm_oEmp_MmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_sentences = brown_corpus"
      ],
      "metadata": {
        "id": "B-FbZey9EXqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_sentences = []\n",
        "Y_tags = []\n",
        "for item in tagged_sentences:\n",
        "  X = []\n",
        "  Y = []\n",
        "  for e in item:\n",
        "    X.append(e[0])\n",
        "    Y.append(e[1])\n",
        "  X_sentences.append(X)\n",
        "  Y_tags.append(Y)"
      ],
      "metadata": {
        "id": "oueXu7q5EqQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_sentences[1], Y_tags[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf-Q8gqSG83Z",
        "outputId": "956345c5-b34d-4993-eb8a-8f29647c8972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['The',\n",
              "  'jury',\n",
              "  'further',\n",
              "  'said',\n",
              "  'in',\n",
              "  'term-end',\n",
              "  'presentments',\n",
              "  'that',\n",
              "  'the',\n",
              "  'City',\n",
              "  'Executive',\n",
              "  'Committee',\n",
              "  ',',\n",
              "  'which',\n",
              "  'had',\n",
              "  'over-all',\n",
              "  'charge',\n",
              "  'of',\n",
              "  'the',\n",
              "  'election',\n",
              "  ',',\n",
              "  '``',\n",
              "  'deserves',\n",
              "  'the',\n",
              "  'praise',\n",
              "  'and',\n",
              "  'thanks',\n",
              "  'of',\n",
              "  'the',\n",
              "  'City',\n",
              "  'of',\n",
              "  'Atlanta',\n",
              "  \"''\",\n",
              "  'for',\n",
              "  'the',\n",
              "  'manner',\n",
              "  'in',\n",
              "  'which',\n",
              "  'the',\n",
              "  'election',\n",
              "  'was',\n",
              "  'conducted',\n",
              "  '.'],\n",
              " ['DET',\n",
              "  'NOUN',\n",
              "  'ADV',\n",
              "  'VERB',\n",
              "  'ADP',\n",
              "  'NOUN',\n",
              "  'NOUN',\n",
              "  'ADP',\n",
              "  'DET',\n",
              "  'NOUN',\n",
              "  'ADJ',\n",
              "  'NOUN',\n",
              "  '.',\n",
              "  'DET',\n",
              "  'VERB',\n",
              "  'ADJ',\n",
              "  'NOUN',\n",
              "  'ADP',\n",
              "  'DET',\n",
              "  'NOUN',\n",
              "  '.',\n",
              "  '.',\n",
              "  'VERB',\n",
              "  'DET',\n",
              "  'NOUN',\n",
              "  'CONJ',\n",
              "  'NOUN',\n",
              "  'ADP',\n",
              "  'DET',\n",
              "  'NOUN',\n",
              "  'ADP',\n",
              "  'NOUN',\n",
              "  '.',\n",
              "  'ADP',\n",
              "  'DET',\n",
              "  'NOUN',\n",
              "  'ADP',\n",
              "  'DET',\n",
              "  'DET',\n",
              "  'NOUN',\n",
              "  'VERB',\n",
              "  'VERB',\n",
              "  '.'])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts(X_sentences)\n",
        "X_encoded = word_tokenizer.texts_to_sequences(X_sentences)"
      ],
      "metadata": {
        "id": "m0LgLWGZ9dut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(Y_tags)\n",
        "Y_encoded = tag_tokenizer.texts_to_sequences(Y_tags)"
      ],
      "metadata": {
        "id": "dUoEvjIA92FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "ujXig8wj-s_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_padded = pad_sequences(X_encoded, maxlen=150, padding=\"pre\", truncating=\"post\")\n",
        "Y_padded = pad_sequences(Y_encoded, maxlen=150, padding=\"pre\", truncating=\"post\")"
      ],
      "metadata": {
        "id": "2myPbdpn_qsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = X_padded, Y_padded"
      ],
      "metadata": {
        "id": "byyeEagk_739"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/GoogleNews-vectors-negative300.bin' #https://drive.google.com/file/d/1S-R94SvCoAAOQYzuLvh5OudF_vFfEl46/view?usp=shar #\n",
        "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
      ],
      "metadata": {
        "id": "FDzOvmfXJrta",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "0ae03ba4-33d6-4764-f98d-815ef23140f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c662cd9d658e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/u/0/shared-with-me/GoogleNews-vectors-negative300.bin'\u001b[0m \u001b[0;31m#https://drive.google.com/file/d/1S-R94SvCoAAOQYzuLvh5OudF_vFfEl46/view?usp=shar #https://drive.google.com/drive/u/0/shared-with-me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \"\"\"\n\u001b[1;32m   1435\u001b[0m         \u001b[0;31m# from gensim.models.word2vec import load_word2vec_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m             limit=limit, datatype=datatype)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mso_compression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFER_FROM_EXTENSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/u/0/shared-with-me/GoogleNews-vectors-negative300.bin'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_weights = np.zeros((len(word_tokenizer.word_index) + 1, 300))\n",
        "word_dict = word_tokenizer.word_index\n",
        "\n",
        "for word, index in word_dict.items():\n",
        "    try:\n",
        "        embedding_weights[index, :] = word_dict[word]\n",
        "    except KeyError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "LuIh7XRvMG8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = to_categorical(Y)"
      ],
      "metadata": {
        "id": "yaja8iqjNQcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "yeBM90xdNhY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=4)"
      ],
      "metadata": {
        "id": "usVpNWp4NZ72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TRAINING DATA\")\n",
        "print('Shape of input sequences: {}'.format(X_train.shape))\n",
        "print('Shape of output sequences: {}'.format(Y_train.shape))\n",
        "print(\"-\"*50)\n",
        "print(\"TESTING DATA\")\n",
        "print('Shape of input sequences: {}'.format(X_test.shape))\n",
        "print('Shape of output sequences: {}'.format(Y_test.shape))"
      ],
      "metadata": {
        "id": "W8cTsNMrNvi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN fixed embeddings"
      ],
      "metadata": {
        "id": "WO4c3zdSOHwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model = Sequential()\n",
        "nn_model.add(Embedding(input_dim= len(word_dict) + 1, output_dim=300,input_length=150,weights = [embedding_weights],trainable=  True ))\n",
        "\n",
        "\n",
        "nn_model.add(Dense(256, input_dim=300))\n",
        "nn_model.add(Activation('relu'))\n",
        "nn_model.add(Dense(13, activation=\"softmax\"))\n",
        "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "nn_model.compile(loss =  'categorical_crossentropy',optimizer =  sgd,metrics   =  ['accuracy'])"
      ],
      "metadata": {
        "id": "NNCDAhzF1uiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model.summary()"
      ],
      "metadata": {
        "id": "WTwt1SNfOV4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_training = nn_model.fit(X_train, Y_train, batch_size=128, epochs=10)"
      ],
      "metadata": {
        "id": "aVQlvoF0OXUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model Accuracy on test set = \", nn_model.evaluate(X_test,Y_test))"
      ],
      "metadata": {
        "id": "qEBVkUZwpNsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "  nn_model = Sequential()\n",
        "  nn_model.add(Embedding(input_dim= len(word_dict) + 1, output_dim=300,input_length=150,weights = [embedding_weights],trainable=  True ))\n",
        "\n",
        "\n",
        "  nn_model.add(Dense(256, input_dim=300))\n",
        "  nn_model.add(Activation('relu'))\n",
        "  nn_model.add(Dense(13, activation=\"softmax\"))\n",
        "  sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  nn_model.compile(loss =  'categorical_crossentropy',optimizer =  sgd,metrics   =  ['accuracy'])\n",
        "  return nn_model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k4oegTMhOPSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#doing kfold cross validation  X_train, X_test, Y_train, Y_test\n",
        "import random\n",
        "k=5\n",
        "num_validation_sample=len(X_train)//k\n",
        "#np.random.shuffle(data)\n",
        "\n",
        "validation_score=[]\n",
        "for fold in range(k):\n",
        "  validation_data=X_train[num_validation_sample*fold:num_validation_sample*(fold+1)] # valiation data\n",
        "  validation_target=Y_train[num_validation_sample*fold:num_validation_sample*(fold+1)]\n",
        "\n",
        "  training_data=X_train[:num_validation_sample*fold]+X_train[num_validation_sample*(fold+1):] #remaining data\n",
        "  training_target=Y_train[:num_validation_sample*fold]+Y_train[num_validation_sample*(fold+1):]\n",
        "\n",
        "  model=build_model()\n",
        "  model.fit(training_data, training_target, batch_size=128, epochs=10)\n",
        "  #model.fit(training_data,training_target,epochs=20,batch_size=100)\n",
        "\n",
        "  validation_score=model.evaluate(validation_data,validation_target)\n",
        "  validation_score.append(validation_score)\n",
        "\n",
        "validation_score=np.average(validation_score)\n",
        "\n",
        "# model=get_model()\n",
        "# model.trian(testing_data)\n",
        "# test_score=model.evaluate(test_data)\n"
      ],
      "metadata": {
        "id": "WWMhlOu3xB0A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}